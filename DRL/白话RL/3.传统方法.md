# 动态规划





# 蒙特卡洛方法







### 增量平均

$V(S_t)=V(S_t)+a（Gt−V(S_t)）.$





<img src="https://gitee.com/matytan/tupic/raw/master/uPic/image-20211109162604567.png" alt="image-2021109162604567" style="zoom:67%;" />

# 时间差分





### SARSA

s->a->r->s->a是一种On policy算法

具体公式：

$Q(s,a) := Q(s,a)+a(R + {\gamma}Q(s',a')-Q(s,a))$

```cpp
初始化函数Q(sa)，令任何一个终止状态的Q值都为0￼ 
  对于每个Episode循环￼      
  设置初始状态S￼       
  根据Q函数与状态S，选择动作A（例如ε−greedy方法）￼       
  对于每个Step循环￼           做一个动作A，观测R和S′￼           
  根据Q函数和状态S′，选择动作A′（例如ε−greedy方法）￼          
  赋值：Q(S,A)←Q(S,A)+a[R+γQ(S′,A′)−Q(S,A)]￼           
  赋值：S←S′,A←A′￼       
  直到S是终止状态
```





### Q-learning

公式：

$Q(s,a) := Q(s,a)+a[R + {\gamma}max_{a'}(Q(s',a'))-Q(s,a)]$



![image-20211109170048238](https://gitee.com/matytan/tupic/raw/master/uPic/image-20211109170048238.png)





## on-policy off-policy





## on-line 和 off-line