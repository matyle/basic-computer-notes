## 历史



1957年 Rosenblatt提出感知器模型

能解决简单的问题



## 神经元



## 线性模型

<img src="https://gitee.com/matytan/tupic/raw/master/uPic/image-20211110145902991.png" alt="image-2021111014592991" style="zoom:50%;" />

```python
# Linear regression model
model = nn.Linear(input_size, output_size)
```





## 激励函数



![image-20211110151608151](https://gitee.com/matytan/tupic/raw/master/uPic/image-20211110151608151.png)





### Sigmoid

![image-20211110151704776](https://gitee.com/matytan/tupic/raw/master/uPic/image-20211110151704776.png)

<img src="https://gitee.com/matytan/tupic/raw/master/uPic/image-20211110151715625.png" alt="image-202111101715625" style="zoom:50%;" />





- Sigmoid函数最终将输出投射到了0-1之间，Sigmoid激励函数通常可以作为概率解释，引入了非线性因素

  > 为什么要用引入非线性因素？

  仅通过线性函数拟合，一定只有线性关系，可能容易造成重欠拟合

### Tanh函数

> 双曲正切函数：投射到-1到1

循环神经网络中经常使用

![image-20211110152628143](https://gitee.com/matytan/tupic/raw/master/uPic/image-20211110152628143.png)

<img src="https://gitee.com/matytan/tupic/raw/master/uPic/image-20211110152707756.png" alt="image-2021111152707756" style="zoom:50%;" />



### ReLU函数

> 卷积神经网络中经常使用的激励函数 Rectified Linear Units

<img src="https://gitee.com/matytan/tupic/raw/master/uPic/image-20211110152820423.png" alt="image-2021110152820423" style="zoom:50%;" />

​													y=max(x,0).

### Linear函数

即 $f(x) = x$



## 神经网络函数

> 神经网络的组成



- 输入层 input layer
  - 直接接收输入的数据，不做数据处理 因此不算层数
- 隐藏层 Hidden Layer
  - 隐藏层层数不定
- 输出层 Output layer：最后一层，输出整个网络处理的值，可以自己构造他需要是什么样的



## 网络训练

> 输入，输出，网络结构，损失函数



### 输入

- 任一形式的输入，需要进行量化形式输入
- 归一化，白桦，灰度化，嵌入（Embedding)才能输入



### 输出

输出的内容和需求直接相关



### 网络结构

每一层网络都相当于一个函数g(x|θ)。

<img src="https://gitee.com/matytan/tupic/raw/master/uPic/image-20211111140958358.png" style="zoom:50%;" />



https://github.com/PKUanonym/REKCARC-TSC-UHT

需要网络正常工作，就需要训练



### 损失函数





MSE Loss 均方损失函数 Mean Squared Error Loss

<img src="https://gitee.com/matytan/tupic/raw/master/uPic/image-20211111141931996.png" alt="image-2021111141931996" style="zoom:67%;" />

xi和yi是已知数，是样本；未知的是θ。





凸函数：

<img src="https://gitee.com/matytan/tupic/raw/master/uPic/image-20211111144303969.png" alt="image-2021144303969" style="zoom:67%;" />

<img src="https://gitee.com/matytan/tupic/raw/master/uPic/image-20211111143846261.png" alt="image-2011111143846261" style="zoom:50%;" />





### 二维凸函数

<img src="https://gitee.com/matytan/tupic/raw/master/uPic/image-20211111144248154.png" alt="image-2114248154" style="zoom:80%;" />

求偏导：

$$x_{n+1}=x_n - \eta \frac{df(x)}{dx}$$



$$y_{n+1}=y_n - \eta \frac{df(y)}{dy}$$

三维空间中 从山顶到山脚，有一个名词叫做==梯度==

<img src="https://gitee.com/matytan/tupic/raw/master/uPic/image-20211111145042689.png" alt="image-2021111114042689" style="zoom:50%;" />

<img src="https://gitee.com/matytan/tupic/raw/master/uPic/image-20211111145148475.png" alt="image-2021111145148475" style="zoom:50%;" />



变量很多怎么办？

<img src="https://gitee.com/matytan/tupic/raw/master/uPic/image-20211111145839017.png" alt="image-2021111145839017" style="zoom:50%;" />

<img src="https://gitee.com/matytan/tupic/raw/master/uPic/image-20211111145902148.png" alt="image-2021111145902148" style="zoom:67%;" />



### 求导

- 连乘型
- 嵌套型







## 全连接神经网络





## 卷积神经网络



卷积核：正方形的框

<img src="https://gitee.com/matytan/tupic/raw/master/uPic/image-20211111161906825.png" alt="image-202111111619065" style="zoom:67%;" />



![image-20211111161926030](https://gitee.com/matytan/tupic/raw/master/uPic/image-20211111161926030.png)

即$y=1×1+0×1+1×1+0×1+1×1+0×1+1×0+0×0+1×1+0=4.$



Feature Map：计算结果存储在Feature map中

在边缘补0 称为padding



扫描：

扫描是可以“跳跃”进行的，也就是说，可以一次移动1个像素后进行扫描，也可以一次移动2个或更多个像素。移动1个像素，就是Striding=1，这是扫描最为密集的方式。

Striding的值可以为2或者更大，值越大，则跳过的信息越多，扫描的速度也就越快，但会牺牲扫描提取的信息量。



kernel_size 表示卷积核的尺寸（大小）

- 通道数

卷积核数量叫做通道数（Channel）。前面两个参数

` nn.Conv2d(1, 16, kernel_size=5, stride=1, padding=2), # 1表示输入通道为1，输出通道为16`

即输入一幅1通道的图片，有16个卷积核来扫描它）





==Max Pooling==



<img src="https://gitee.com/matytan/tupic/raw/master/uPic/image-20211111163514143.png" alt="image-2021111163514143" style="zoom:70%;" />



```python
            nn.Conv2d(1, 16, kernel_size=5, stride=1, padding=2), # 1表示输入通道为1，输出通道为16
            nn.BatchNorm2d(16),
            nn.ReLU(),
            nn.MaxPool2d(kernel_size=2, stride=2))
```





## LSTM

<img src="https://gitee.com/matytan/tupic/raw/master/uPic/image-20211112152226413.png" alt="image-20211111226413" style="zoom:50%;" />

从左到右，会有一个向量进行传输。它从单元的左侧输入，称作Ct−1；从单元的右侧输出，称作Ct。在这里，只有两个交互的部分，一个是左边的“乘号”，另一个是右边的“加号”。先说“加号”部分，就是普通的向量线性叠加。左边的“乘号”是一个乘法器，这个操作相当于左侧的Ct−1进入单元后，先被一个乘法器乘以一个系数，再线性叠加一个值，然后从右侧输出。



$f_t=σ(Wf[h_t−1,x_t]+b_f).$

<img src="https://gitee.com/matytan/tupic/raw/master/uPic/image-20211112152315093.png" alt="image-2021111215215093" style="zoom:67%;" />

$i_t=σ(W_i[h_t−1,x_t]+b_i).$

<img src="https://gitee.com/matytan/tupic/raw/master/uPic/image-20211112152429750.png" alt="image-2021111212429750" style="zoom:67%;" />



<img src="https://gitee.com/matytan/tupic/raw/master/uPic/image-20211112152839412.png" alt="image-2021111252839412" style="zoom:67%;" />

<img src="https://gitee.com/matytan/tupic/raw/master/uPic/image-20211112152907240.png" alt="image-2021111215207240" style="zoom:67%;" />

RNN使得计算机能有一个推理的能力







## 梯度消失和梯度爆炸

1. 梯度下降
2. 随机梯度下降：随机选batch更新样本
