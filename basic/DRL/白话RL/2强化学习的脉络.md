//一周内完成这本书的阅读，提取重要的知识作为毕业论文前几章

//然后后序研究如何让智能体利用元学习学习到如何探索，增强好奇心作为后面章节

## 什么是策略

policy策略：$a = \pi(s)$

其中a是动作，s为状态。策略类似于一个函数

$a = \pi(s|{\theta})$ ,其中$\theta$表示训练的参数



- 输出概率：输出的a不是确定的值，而是一个概率分布

​	$\pi(s)=P[A_t=a|S_t=s]$

  表示输入一个状态$S_t=s$的时候，应该输出的动作$A_t=a$的概率为多少



## 如何评估策略的好坏

如何评价策略的好坏，我们可能有个评价函数Evaluation Function

机器学习中只能比较“数”的大小。考试就是Environment（环境），让不同的策略在环境中不断摸爬滚打，最后计算谁的分高。

## 什么是模型



输入一个状态St=s的时候，应该输出的动作At=a的概率为多少



- 关于P

  <img src="https://gitee.com/matytan/tupic/raw/master/uPic/image-20211109144919176.png" alt="image-2021109144919176" style="zoom:50%;" />

<img src="https://gitee.com/matytan/tupic/raw/master/uPic/image-20211109144834650.png" alt="image-2021109144834650" style="zoom:50%;" />



- 关于R

<img src="https://gitee.com/matytan/tupic/raw/master/uPic/image-20211109144853518.png" alt="image-2021110944853518" style="zoom:20%;" />

![image-20211109144827705](https://gitee.com/matytan/tupic/raw/master/uPic/image-20211109144827705.png)



### 直接法估计

r=R(π(s|θ)|s).

<img src="https://gitee.com/matytan/tupic/raw/master/uPic/image-20211113143849544.png" alt="image-20211113143849544" style="zoom:50%;" />



### 间接法估计

我们只有这几个因素：$s_t,a,s_{t+1},r$ ,能不能用瞬时奖励r作为比较对象，哪个大就选哪个？

value函数，值函数估计

$vπ(s)=Eπ[Rt+1+γRt+2+γ2Rt+3+...|St=s]$.



## 马尔科夫决策过程

在一系列事件中，某给定事件发生的概率，只取决于前面发生的事件。言外之意就是：更久以前发生的事件不在研究范围内，只关注前面发生的事件，只针对前面发生的事件和现在发生的事件的关系来做研究。这里的“前面”，指的就是时序上的“先”。

### 状态转移

马尔科夫过程必须满足条件：

P[St+1|St]=P[St+1|S1,...,St].这个等式的含义是，前一个状态St到后一个状态St+1的转移概率，不会随着更久以前的状态的加入而改变。



### 策略评价

某个状态的价值：

​	计算方法1：$vπ(s)=Eπ[Rt+1+γRt+2+γ2Rt+3+...|St=s].$



<img src="https://gitee.com/matytan/tupic/raw/master/uPic/image-20211109152742326.png" alt="image-2021110952742326" style="zoom:50%;" />

例如：vπ（晴）=0.5×qπ（晴，打伞）+0.5×qπ（晴，不打伞）.





qπ(s,a)表示，在策略π的驱使下，以状态s做动作a的估值（这个估值就是价值估值），q是站在动作的角度，对动作的估值（没有二义性）

<img src="https://gitee.com/matytan/tupic/raw/master/uPic/image-20211109152956633.png" alt="image-2021110915956633" style="zoom:50%;" />

qπ（晴，打伞）=（0.7×1+0.3×(−1)）+0.9×（0.7×vπ（晴）+0.3×vπ（雨））. 其中：

​	（0.7×1+0.3×(−1)）这一部分是现实回报，而后面是长远回报



用$q_{\pi}(s,a)$替换掉：

<img src="https://gitee.com/matytan/tupic/raw/master/uPic/image-20211109153046254.png" alt="image-2021110915046254" style="zoom:50%;" />

### 策略优化

策略迭代：

1. 尝试执行某个策略得到响应的vπ(s)
2. 