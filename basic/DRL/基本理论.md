

深度学习：分类器在训练之后能够将不同的动物或者分类，但是过了一段时间中，很多之前的物品随着时间变化和之前有了很多改变，此时分类器可能不能够运行的非常好了。

强化学习：引入了额外的维度（通常是时间维度，并非必须是时间），因此更接近真正的人工智能。

要素：老鼠，迷宫，电击（或是蛋糕）



\## 强化学习的复杂性

\1. 观察的结果--->决定于动作，而结果本身不会告诉智能体哪里做错了，并且如何执行才能改善结果，样本的非i.i.d 独立同分布数据，iid是大多数监督学习的前提

\2. 智能体不仅要利用学的知识，还要积极探索环境。多的探索（过多的不同动作）可能会明显改善结果，但是过多的探索可能会降低奖励（而且有忘记之前学的知识），所以如何平衡探索和利用是非常重要的？（论文的重点）

\3. 选择动作的奖励严重延迟。

 

 

\## 强化学习的形式

 

\### 奖励

对智能体的一种反馈，告诉它有多成功，也是强化学习的核心

局部：智能体最近一段时间有多成功，而不是开始到现在累积的行为有多成功

 

 

金融交易:对买卖股票的交易者来说，奖励就是收益的多少。 

国际象棋:奖励在游戏结束时以赢、输或平局的形式获得。当然，这也取决于平台。例如，对我来说，能与国际象棋大师打平 就算巨大的奖励。实际上，我们需要指定奖励的具体值，但这可 能会是一个相当复杂的表达式。例如，在国际象棋中，奖励可能 与对手的强弱成比例。 

大脑中的多巴胺系统:大脑中有一块区域(边缘系统)会在每次 需要给大脑的其他部分发送积极信号时释放多巴胺。高浓度的多 巴胺会使人产生愉悦感，从而加强此系统认为好的行为。不幸的 是，边缘系统比较“过时”，它会认为食物、繁殖和支配是好 的，但这又是另外一个故事了。 

电脑游戏:玩家总是能得到很明显的反馈，即杀死敌人的数量或 获得的分数。注意，在这个例子中，反馈已经被累积了，所以街 机游戏的RL奖励应该是分数的导数，即新的敌人被杀时是1，其他 时候都是0。 

网页浏览:存在一些有很高实用价值的问题，即需要对网页上可 用的信息进行自动抽取。搜索引擎通常就是为了解决这个问题， 但有时，为了获得正在寻找的数据，需要填一些表单，浏览一系 列链接或输入验证码，而这对于搜索引擎来说是很困难的事。有 一种基于RL的方法可以处理这些任务，奖励就是你想获得的信息 或结果。

 

神经网络(Neural Network，NN)结构搜索:RL已成功应用于NN 结构优化领域，它的目标是通过一些手段在一些数据集中获得最 佳性能，这些手段通常包括调整网络的层数或参数、添加额外的 残差连接，或对NN结构做出其他改变。这种情况下，奖励就是NN 的性能(准确性或其他能衡量NN预测是否精准的度量)。

\### 环境

环境交互部分：奖励，动作以及观察(observe)

 

\### 动作

离散动作和连续动作。离散动作构成了智能体可以做的互斥的有限集合，例如向左移动或向右移 动。连续动作会涉及数值，例如汽车转动方向盘的动作在操作上就涉 及角度和方向的数值。不同的角度可能会导致一秒后的情况有所不 同，所以只转动方向盘肯定是不够的

 

\### 观察

智能体的第二个信息渠道（第一个信息渠道是奖励）

 

例子：

 

金融交易:在这里，环境指整个金融市场和所有影响它的事物。 这涵盖非常多的事情，例如最近的新闻、经济和政治状况、天 气、食物供应和推特趋势。甚至你今天决定待在家里的决定也可 能会间接影响世界的金融系统(如果你相信“蝴蝶效应”的 话)。然而，我们的观察仅限于股票价格、新闻等。我们无法查 看环境中的大部分状态(是它们使金融交易变得如此复杂)。

 

国际象棋:这里的环境是你的棋盘加上你的对手，包括他们的棋 艺、心情、大脑状态、选择的战术等。观察就是你看到的一切 (当前棋子的位置)，但是，在某些级别的对决中，心理学的知 识和读懂对手情绪的能力可以增加你获胜的概率。 

大脑中的多巴胺系统:这里的环境是你的大脑、神经系统、器官 加上你能感知的整体世界。观察是大脑内部的状态和来自感官的 信号。 

电脑游戏:这里的环境是你的计算机的状态，包括所有内存和磁 盘数据。对于网络游戏来说，还包括其他计算机以及它们与你的 计算机之间的所有互联网基础设施。观察则只是屏幕中的像素和 声音。这些像素信息并不是小数量级的(有人计算过，取中等大 小图片(1024×768)的像素进行排列组合，其可能结果的数量明 显大于我们星系中的原子数量)，但整个环境的状态的数量级肯 定更大。 

网页浏览:这里的环境是互联网，包括我们的工作计算机和服务 器计算机之间的所有网络基础设施，包含了数百万个不同的组 件。观察则是当前浏览步骤中加载的网页。 

NN结构搜索:在本例中，环境相当简单，包括执行特定NN评估的 工具集和用于获取性能指标的数据集。与互联网相比，它就像个 玩具环境。观察则不同，它包括关于测试的一些信息，例如损失 函数的收敛状态或者能从评估步骤中获得的其他指标。

 

 

RL中的不同领域，有许多领域都与RL有关。最重要的一些领域如图1.3所示，其中包 括6个在方法和特定主题方面都有大量重叠的大型领域，它们都与决策 相关(显示在灰色圆圈内

![img](../../../../../../private/var/folders/48/gzc0_w3n1m76chdsv6z9h9080000gn/T/com.kingsoft.wpsoffice.mac/wps-matytan/ksohtml/wpsDWHZmq.jpg) 

ML:RL是ML的分支，它从ML借鉴了许多机制、技巧和技术。基本 上，RL的目标是在给定不完整观察数据的情况下，学习智能体的最优行动。 

工程学(尤其是最优控制):帮助RL识别如何采取一系列最优动作来获得最佳结果。 

神经科学:以多巴胺系统为例，说明人脑的行为和RL模型很类似。 

心理学:心理学研究在各种条件下的行为，例如人们对环境的反 应和适应方式，这与RL的主题很接近。

 经济学:经济学的一个重要主题是，如何在知识不完善以及现实 世界不断变化的条件下，最大化奖励。 

数学:适用于理想化的系统，同时也致力于运筹学领域中最优条 件的寻找和实现。







 

然后将其扩展成马尔可夫奖励 过程(Markov reward process)，在最后本文加入动作的概念，得到马尔科夫决策过程MDP

 

\#### MP 马尔科夫过程

系统中所有可能的状态形成了一个集合，称为状态空间，观察 结果形成了一个状态序列或状态链(这就是MP也称为马尔可夫链的原 因)

例如城市天气模型[晴天，雨天，雨天，晴天....]称为历史

 

要将这样的系统称为MP，它需要满足马尔可夫性质 

即系统未来的任何状态变化都仅依赖于当前状态

 

在天气的例子中，马尔可夫性质将模型限制在这样的情况下:不管之前看到了多少个晴天，晴天之后是雨天的概率都是相同的。这个模型不太现实，因为根据常识，我们知道明天会下雨的概率不仅取决于当前的状况，还取决于许多其他因素，例如季节、纬度以及附近是否有山和海。最近有研究表明，甚至连太阳的活动都会对天气造成重大影响。所以，这个例子的假设有点太天真了，但是有助于理解限制条件并做出清醒的决定。

当然，如果想让模型变得更复杂，只需要扩展状态空间就可以了，以更大的状态空间为代价，我们可以捕获模型更多的依赖项。例如，如果要分别捕获夏天和冬天雨天的概率，将季节包含在状态中即可。在这种情况下，状态空间将是[晴天+夏天，晴天+冬天，雨天+夏 天，雨天+冬天]。

 

MP的正式定义：

一组状态(S)，系统可以处于任一状态。 

一个转移矩阵(T)，通过转移概率定义了系统的动态。

 

 还需要注意，马尔可夫性质暗示了稳定性(即所有状态的底层转移概率分布不会随着时间变化)。非稳定性意味着有一些隐藏的因素在影响系统的动态，而这些因素没有被包含在观察中。但是，这与马尔可夫性质相矛盾，后者要求同一状态的底层概率分布必须相同，和

状态的转移历史无关。

 

 

\### 马尔科夫奖励过程

 

首先，在状态转移之间 引入价值的概念。概率已经有了，但是概率是用来捕获系统的动态 的，所以现在在没有额外负担的情况下，添加一个额外的标量。

 

所以现在在没有额外负担的情况下，添加一个额外的标量。奖励能用各种形式来表示。最常用的方法是增加一个方阵，和转移矩阵相似，用i行j列来表示从状态i转移到状态j的奖励。

 

 如前所述，奖励可正可负、可大可小。在某些情况下，这种表示方法是多余的，可以被简化。例如，如果不管起始状态是什么，奖励都是由到达状态给出的，那么可以只保留状态→奖励对，这样的表示更紧凑。但情况并不总是这样，它要求奖励必须只依赖于目标状态。

 

加入的第二个东西是折扣因子γ，它是从0到1(包 含0和1)的某个数字

 

我们会观察一个MP的状态转移链。在马尔可夫奖励 过程中也是这样，但是每次转移，都加入一个新的量——奖励。现 在，每次系统状态转移时，我们的观察都会加入一个奖励值。

对于每一个片段，t时刻的回报定义如下:





首先，在状态转移之间 引入价值的概念。概率已经有了，但是概率是用来捕获系统的动态 的，所以现在在没有额外负担的情况下，添加一个额外的标量。

 

所以现在在没有额外负担的情况下，添加一个额外的标量。奖励能用各种形式来表示。最常用的方法是增加一个方阵，和转移矩阵相似，用i行j列来表示从状态i转移到状态j的奖励。

 

 如前所述，奖励可正可负、可大可小。在某些情况下，这种表示方法是多余的，可以被简化。例如，如果不管起始状态是什么，奖励都是由到达状态给出的，那么可以只保留状态→奖励对，这样的表示更紧凑。但情况并不总是这样，它要求奖励必须只依赖于目标状态。





